{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b83a224c",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Capstone Project\n",
    "\n",
    "https://capstone-project.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd5566",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "- [Problem Statement](#Problem-Statement)\n",
    "- [About Retrieval Augmented Generation (RAG) and Fine-tuning](#About-Retrieval-Augmented-Generation-(RAG)-and-Fine-tuning)\n",
    "- [RAG](#RAG)\n",
    "- [Fine-tuning](#Fine-tuning)\n",
    "- [Evaluation](#Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c944ea8a",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "This project seeks to recommend authentic restaurants in Singapore based on consumer preferences and to support rstaurants in enhancing the authenticity and quality of their offerings through customer feedback analysis, including key words, topics, and Net Promoter Scores (NPS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d4e761",
   "metadata": {},
   "source": [
    "## About Retrieval Augmented Generation (RAG) and Fine-tuning\n",
    "\n",
    "In this project, I used the architecture that powers ChatGPT, a generative AI tool that has revolutionized the way users get answers to their questions. To build a custom chatbot using ChatGPT's Large Language Model (LLM), two techniques, Retriever Augmented Generation (RAG) and model Fine-tuning is used. \n",
    "\n",
    "RAG adjusts knowledge the LLM has access to through external knowledge retrieval, while fine-tuning adjusts the behaviour of the LLM for specific domains by training it on specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc96d46",
   "metadata": {},
   "source": [
    "## Import libraries, API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9888248b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import Document, GPTVectorStoreIndex, ServiceContext\n",
    "from llama_index import SimpleDirectoryReader # For PDF and HTML\n",
    "from llama_index import download_loader # For CSV\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.evaluation import DatasetGenerator\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09accb31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load env var\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75d25c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## [Optional] Create a separate csv file for each entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def split_rows_by_column(input_file, output_folder, encoding='utf-8'):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Read the input CSV file\n",
    "    with open(input_file, 'r', newline='', encoding=encoding) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "\n",
    "        # Skip the header row if it exists\n",
    "        header = next(reader, None)\n",
    "\n",
    "        # Dictionary to store file writers for each unique value in Column 1\n",
    "        file_writers = {}\n",
    "\n",
    "        # Iterate through rows and write them to separate CSV files\n",
    "        for row in reader:\n",
    "            key = row[0]  # Assuming Column 1 is at index 0\n",
    "            if key not in file_writers:\n",
    "                # Create a new CSV file for each unique value in Column 1\n",
    "                file_path = os.path.join(output_folder, f'{key}_output.csv')\n",
    "                file_writers[key] = csv.writer(open(file_path, 'w', newline='', encoding=encoding))\n",
    "                \n",
    "                # Write the header to the new CSV file\n",
    "                if header:\n",
    "                    file_writers[key].writerow(header)\n",
    "\n",
    "            # Write the current row to the appropriate CSV file\n",
    "            file_writers[key].writerow(row)\n",
    "\n",
    "# Example usage\n",
    "input_csv_file = 'data/authentic-restaurants-reviews.csv'\n",
    "output_folder = 'data'\n",
    "\n",
    "split_rows_by_column(input_csv_file, output_folder)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1853e04",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99211f6b",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "[LlamaIndex's documentation](https://gpt-index.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "\n",
    "LlamaIndex loads data via data connectors. The easiest and most commonly used data connector is the `SimpleDirectoryReader`, which creates documents out of every file in the given directory, 'webpages' in this case. 'webpages' consists of data of various entities saved from their respective websites in **PDF and html format**.\n",
    "<br>A `Document` is a collection of text data and metadata about that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this code if your data set is in PDF / HTML format\n",
    "'''\n",
    "filename_fn = lambda filename: {'file_name': filename}\n",
    "reader = SimpleDirectoryReader('data', exclude_hidden=True, file_metadata=filename_fn)\n",
    "docs = reader.load_data()\n",
    "print([x.doc_id for x in docs])\n",
    "print(f\"Loaded {len(docs)} docs\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbcb5f78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 40946 docs\n"
     ]
    }
   ],
   "source": [
    "#Use this code if your data set is in CSV format\n",
    "PagedCSVReader = download_loader(\"PagedCSVReader\")\n",
    "\n",
    "loader = PagedCSVReader(encoding=\"utf-8\")\n",
    "docs = loader.load_data(file=Path('data/restaurant_review_score.csv'))\n",
    "\n",
    "# print([x.doc_id for x in docs])\n",
    "print(f\"Loaded {len(docs)} docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba98350",
   "metadata": {},
   "source": [
    "### Indexing and Storage\n",
    "\n",
    "With all the data loaded, a list of [N] Document objects is created. Proceed to build an Index over these objects, which makes it ready for querying by an LLM. There are 4 types of indexing: Summary index, VectorStore Index, Tree Index and Keyword Table Index. \n",
    "\n",
    "In this project, `VectorStoreIndex` is used as it is by far the most frequent type of indexing. It takes the Documents and splits them up into Nodes, then creates `vector embeddings` of the text of every node. `Vector embedding` aka embedding is a numerical representation of the semantics, or meaning of the text. Two pieces of text with similar meanings will have mathematically similar embeddings, even if the actual text is quite different. This mathematical relationship enables semantic search, where a user provides query terms and LlamaIndex can locate text that is related to the meaning of the query terms rather than simple keyword matching. This is a big part of how Retrieval-Augmented Generation works.\n",
    "\n",
    "Definition of classes:\n",
    "- `ServiceContext` is a bundle of configuration data which can be passed to other stages of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5884ea5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the LLM (gpt-3.5-turbo) from OpenAI and pass it to ServiceContext().\n",
    "# The GPTVectorStoreIndex will use gpt-3.5-turbo as embedding model to index the documents\n",
    "service_context = ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0)) # degree of randomness from 0 to 1.  \n",
    "index = GPTVectorStoreIndex.from_documents(documents=docs, service_context=service_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809dfbf8",
   "metadata": {},
   "source": [
    "After indexing, the ouput is stored in disk using the built-in .persist() method to avoid the time and cost of having to re-index it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a86885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"data/index.vecstore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d7bea1",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "Currently, there are three key integrations with [LlamaIndex for fine-tuning](https://gpt-index.readthedocs.io/en/latest/optimizing/fine-tuning/fine-tuning.html).\n",
    "\n",
    "In this project, since GPT-3.5-Turbo is used, I will try to distill a better model (e.g. GPT-4) into the simpler/cheaper model (e.g. GPT-3.5), i.e. finetuning GPT-3.5-turbo to ouput GPT-4 responses. \n",
    "\n",
    "The key steps are:\n",
    "1) Split the documents into train/evaluation set\n",
    "1) Generate a question/answer dataset over the train set\n",
    "    - use GPT-3.5-Turbo to generate questions from the external data, and GPT-4 query engine to generate answers.\n",
    "    - `OpenAIFineTuningHandler` callback automatically logs questions/answers to a dataset.\n",
    "2) Launch a finetuning job with `OpenAIFinetuneEngine`, and get back a finetuned model\n",
    "3) Evaluate the performance of the finetuned model and compare with the base model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4f3a75",
   "metadata": {},
   "source": [
    "### Generate Train Dataset\n",
    "\n",
    "This dataset is for finetuning the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b839c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the documents\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(docs)\n",
    "\n",
    "gpt_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac047c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid RuntimeError: asyncio.run() cannot be called from a running event loop\n",
    "# The below code is to unblock: nest the event loops\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c4976ad-db7f-4280-81f8-18a35dbe8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen_query = (\n",
    "    \"You are an expert in finding high quality authentic food. \\\n",
    "    Your task is to ask questions about authentic food to test fellow expert in authentic food. \\\n",
    "    Using the provided context from documents on different restaurants' reviews and ratings, \\\n",
    "    formulate questions about authentic food that captures important facts from the context. \\\n",
    "    Restrict the questions to the context information provided.\"\n",
    ")\n",
    "\n",
    "# find out more about question generation from \n",
    "# https://gpt-index.readthedocs.io/en/latest/examples/evaluation/QuestionGeneration.html\n",
    "\n",
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    docs[:40],\n",
    "    question_gen_query=question_gen_query,\n",
    "    service_context=gpt_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56238a0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated  40  questions\n"
     ]
    }
   ],
   "source": [
    "questions = dataset_generator.generate_questions_from_nodes(num=40)\n",
    "print(\"Generated \", len(questions), \" questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "addee976",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train_questions.txt\", \"w\") as f:\n",
    "    for question in questions:\n",
    "        f.write(question + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eafadd",
   "metadata": {},
   "source": [
    "![train_questions.png](train_questions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a366a4bd",
   "metadata": {},
   "source": [
    "### Generate Evaluation Dataset\n",
    "\n",
    "This dataset is for subsequent evaluation step to measure the performance of the models.\n",
    "<br> Questions are generated from a different set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b0e58bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_generator = DatasetGenerator.from_documents(\n",
    "    docs[\n",
    "        40:80\n",
    "    ],  # since we generated question for the first 40 documents, we can skip the first 40\n",
    "    question_gen_query=question_gen_query,\n",
    "    service_context=gpt_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f50d7307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated  20  questions\n"
     ]
    }
   ],
   "source": [
    "questions = dataset_generator.generate_questions_from_nodes(num=20)\n",
    "print(\"Generated \", len(questions), \" questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c62a9eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"eval_questions.txt\", \"w\") as f:\n",
    "    for question in questions:\n",
    "        f.write(question + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55da467b",
   "metadata": {},
   "source": [
    "![Screenshot of eval generated](eval_questions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a66544e",
   "metadata": {},
   "source": [
    "### GPT-3.5 to Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86d9b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.callbacks import OpenAIFineTuningHandler\n",
    "from llama_index.callbacks import CallbackManager\n",
    "\n",
    "finetuning_handler = OpenAIFineTuningHandler()\n",
    "callback_manager = CallbackManager([finetuning_handler])\n",
    "\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0),\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    "    callback_manager=callback_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e69c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"train_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db75f40b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(docs, service_context=gpt_35_context)\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39ee8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "    response = query_engine.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b1c6ac",
   "metadata": {},
   "source": [
    "### Create `OpenAIFinetuneEngine`\n",
    "\n",
    "`OpenAIFinetuneEngine` is a finetune engine that will take care of launching a finetuning job, and returning an LLM model that can be directly plugged in to the rest of LlamaIndex workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "270f5516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 40 examples to finetuning_events.jsonl\n"
     ]
    }
   ],
   "source": [
    "finetuning_handler.save_finetuning_events(\"finetuning_events.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2050e1a-7380-46b1-8fb6-fa61fe71a006",
   "metadata": {},
   "source": [
    "I finetuned my jsonl file through the OpenAI page to be used for evaluation below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f155643",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0324ad69",
   "metadata": {},
   "source": [
    "To measure the performance of the pipeline, whether it is able to generate relevant and accurate responses given the external data source and a set of queries, we use 2 evaluation metrics from [`ragas` evaluation library](https://github.com/explodinggradients/ragas/tree/main/docs/concepts/metrics). Ragas uses LLMs under the hood to compute the evaluations.\n",
    "\n",
    "The performance of the base model, gpt-3.5-turbo, will be compared with the fine-tuned model.\n",
    "\n",
    "Computation of evaluation metrics require 3 components: \n",
    "1) `Question`: A list of questions that could be asked about my external data/documents, generated using .generate_questions_from_nodes in above fine-tuning step<br>\n",
    "2) `Context`: Retrieved contexts corresponding to each question. The context represents (chunks of) documents that are relevant to the question, i.e. the source from where the answer will be generated.<br>\n",
    "3) `Answer`: Answer generated corresponding to each question from baseline and fine-tuned model.\n",
    "\n",
    "The two metrics are as follow:\n",
    "\n",
    "- `answer_relevancy` - Measures how relevant the generated answer is to the question, where an answer is considered relevant when it <u>directly</u> and <u>appropriately</u> addresses the orginal question, i.e. answers that are complete and do not include unnecessary or duplicated information. The metric does not consider factuality. It is computed using `question` and `answer`, with score ranging between 0 and 1, the higher the score, the better the performance in terms of providing relevant answers. To calculate this score, the LLM is prompted to generate an appropriate question for the generated answer multiple times, and the mean cosine similarity between these generated questions and the original question is measured. The underlying idea is that if the generated answer accurately addresses the initial question, the LLM should be able to generate questions from the answer that align with the original question, i.e. high mean cosine similarity, translating to high score.\n",
    "\n",
    "\n",
    "- `faithfulness` - Measures how factually accurate is the generated answer, i.e. if the response was hallucinated, or based on factuality (from the context). It is computed from `answer` and `context`, with score ranging between 0 and 1, the higher the score, the better the performance in terms of providing contextually accurate information. To calculate this score, the LLM identifies statements within the generated answer and verifies if each statement is supported by the retrieved context. The process then counts the number of statements within the generated answer that can be logically inferred from the context, and dvide by the total number of statements in the answer. \n",
    "\n",
    "Additional note: Cosine similarity is a metric used to measure how similar two items are. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The output value ranges from 0–1 where 0 means no similarity, whereas 1 means that both the items are 100% similar.\n",
    "<br>Hallucinations refer to instances where the language model produces information or claims that are not accurate or supported by the input context.\n",
    "\n",
    "Resources:\n",
    "<br>https://cobusgreyling.medium.com/rag-evaluation-9813a931b3d4\n",
    "<br>https://blog.langchain.dev/evaluating-rag-pipelines-with-ragas-langsmith/\n",
    "<br>https://medium.aiplanet.com/evaluate-rag-pipeline-using-ragas-fbdd8dd466c1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10c1798",
   "metadata": {},
   "source": [
    "### Evaluation of base model: GPT-3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "849d3471",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "with open(\"eval_questions.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        questions.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78c5f581",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "# limit the context window to 2048 tokens so that refine is used\n",
    "gpt_context = ServiceContext.from_defaults(\n",
    "    # If finetuning on openai website, replace the model name accordingly\n",
    "    llm=OpenAI(model=\"ft:gpt-3.5-turbo-0613:personal::8SUkthR5\", temperature=0), context_window=2048\n",
    "    \n",
    "    # If finetuning on localhost, uncomment this code\n",
    "    # llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0), context_window=2048\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(docs, service_context=gpt_context)\n",
    "\n",
    "# as_query_engine builds a default retriever and query engine on top of the index\n",
    "# We configure the retriever to return the top 2 most similar documents, which is also the default setting\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db0acae0-4e9b-4e23-856b-0878b05d8067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "for question in questions:\n",
    "    response = query_engine.query(question)\n",
    "    contexts.append([x.node.get_content() for x in response.source_nodes])\n",
    "    answers.append(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d27a0ad1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [faithfulness]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:22<00:00, 41.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating with [answer_relevancy]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:31<00:00, 15.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'faithfulness': 0.9125, 'answer_relevancy': 0.9643}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness\n",
    "\n",
    "ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "    }\n",
    ")\n",
    "\n",
    "result = evaluate(ds, [faithfulness, answer_relevancy])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782afdb",
   "metadata": {},
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a9de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response.notebook_utils import display_response\n",
    "\n",
    "query_engine = index.as_query_engine(service_context=gpt_context)\n",
    "\n",
    "response = query_engine.query(questions[12])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfbfcb5",
   "metadata": {},
   "source": [
    "### Fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579b2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_context = ServiceContext.from_defaults(\n",
    "    llm=ft_llm,\n",
    "    context_window=2048,  # limit the context window artifically to test refine process\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(service_context=ft_context)\n",
    "\n",
    "response = query_engine.query(questions[12])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b54718",
   "metadata": {},
   "source": [
    "The original base model generated additional sentence \"They will provide you with further information on how to fulfill the wishlist items and make your generous contributions.\" in the answer. This is considered unnecessary information as it is not directly related to the query. The generated answer by the fine-tuned model is concise and sufficiently informative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62622a8",
   "metadata": {},
   "source": [
    "[Click for app code in streamlit](../streamlit/restaurantbot.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
